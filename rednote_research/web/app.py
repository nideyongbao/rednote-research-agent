"""FastAPI Webåº”ç”¨ - æä¾›SSEå®æ—¶ç ”ç©¶ç•Œé¢"""

import os
import asyncio
from pathlib import Path
from datetime import datetime
from typing import Optional
from contextlib import asynccontextmanager

from fastapi import FastAPI, Query, HTTPException
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from sse_starlette.sse import EventSourceResponse
from pydantic import BaseModel

from ..config import Config
from ..state import ResearchState
from ..mcp.rednote import RedNoteMCPClient
from ..agents.orchestrator import ResearchOrchestrator
from ..output.html_generator import HTMLReportGenerator
from ..services.settings import get_settings_service, Settings


# å…¨å±€çŠ¶æ€
_orchestrator: Optional[ResearchOrchestrator] = None
_mcp_client: Optional[RedNoteMCPClient] = None
_config: Optional[Config] = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global _orchestrator, _mcp_client, _config
    
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    _config = Config.from_env()
    
    # MCPå®¢æˆ·ç«¯è·¯å¾„ï¼ˆä»ç¯å¢ƒå˜é‡è·å–ï¼‰
    mcp_path = os.getenv("REDNOTE_MCP_PATH", "")
    if mcp_path:
        # æ”¯æŒç›¸å¯¹è·¯å¾„ï¼šè‡ªåŠ¨è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        if not os.path.isabs(mcp_path):
            # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆrednote_research çš„çˆ¶ç›®å½•ï¼‰
            project_root = Path(__file__).parent.parent.parent
            mcp_path = str((project_root / mcp_path).resolve())
        _mcp_client = RedNoteMCPClient(mcp_path)
        _orchestrator = ResearchOrchestrator(_config, _mcp_client)
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†
    if _mcp_client:
        await _mcp_client.disconnect()


def create_app() -> FastAPI:
    """åˆ›å»ºFastAPIåº”ç”¨"""
    app = FastAPI(
        title="RedNote Research Agent",
        description="åŸºäºMCPçš„å°çº¢ä¹¦æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“",
        version="0.1.0",
        lifespan=lifespan
    )
    
    # æŒ‚è½½é™æ€æ–‡ä»¶ï¼ˆç”Ÿäº§ç¯å¢ƒï¼šå‰ç«¯æ„å»ºäº§ç‰©ï¼‰
    static_dir = Path(__file__).parent / "static"
    if static_dir.exists():
        assets_dir = static_dir / "assets"
        if assets_dir.exists():
            app.mount("/assets", StaticFiles(directory=str(assets_dir)), name="assets")
    
    # æ³¨æ„ï¼šSPA catch-all è·¯ç”±åœ¨æ–‡ä»¶æœ«å°¾æ³¨å†Œï¼Œç¡®ä¿ API è·¯ç”±ä¼˜å…ˆåŒ¹é…
    return app


app = create_app()



@app.get("/api/research")
async def research_stream(topic: str = Query(None), task: str = Query(None, min_length=2)):
    """
    SSEæµå¼è¿”å›ç ”ç©¶è¿›åº¦å’Œç»“æœ
    
    äº‹ä»¶ç±»å‹:
    - message: JSONæ ¼å¼çš„æ¶ˆæ¯ï¼ŒåŒ…å« type, level, message, stage, stats ç­‰å­—æ®µ
    
    æ¶ˆæ¯ç±»å‹(type):
    - log: è¿›åº¦æ—¥å¿—
    - stage: é˜¶æ®µåˆ‡æ¢
    - stats: ç»Ÿè®¡æ›´æ–°
    - complete: å®Œæˆä¿¡å·
    - error: é”™è¯¯ä¿¡æ¯
    """
    import json
    global _orchestrator, _mcp_client, _config
    
    # å…¼å®¹æ–°æ—§å‚æ•°å
    research_topic = topic or task
    if not research_topic or len(research_topic) < 2:
        raise HTTPException(status_code=400, detail="è¯·æä¾›æœ‰æ•ˆçš„ç ”ç©¶ä¸»é¢˜")
    
    # åˆ›å»ºå†å²è®°å½•
    from ..services.history import get_history_service
    history_service = get_history_service()
    record = history_service.create(research_topic)
    record_id = record.id
    
    async def event_generator():
        stats = {"notesFound": 0, "contentsAnalyzed": 0, "insightsExtracted": 0}
        final_status = "failed"  # é»˜è®¤å¤±è´¥ï¼ŒæˆåŠŸæ—¶æ›´æ–°
        
        def make_msg(msg_type: str, **kwargs) -> dict:
            """ç”Ÿæˆæ ‡å‡†æ¶ˆæ¯æ ¼å¼"""
            return {"data": json.dumps({"type": msg_type, "recordId": record_id, **kwargs}, ensure_ascii=False)}
        
        try:
            # æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
            history_service.update(record_id, {"status": "running"})
            
            yield make_msg("log", level="info", message=f"ğŸš€ å¼€å§‹ç ”ç©¶: {research_topic}")
            yield make_msg("stage", stage="planning")
            
            # æ£€æŸ¥MCPå®¢æˆ·ç«¯
            if not _mcp_client:
                yield make_msg("log", level="warning", message="MCPå®¢æˆ·ç«¯æœªé…ç½®ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                # æ¨¡æ‹Ÿç ”ç©¶æµç¨‹
                yield make_msg("log", level="info", message="ğŸ“‹ [Planner] åˆ†æç ”ç©¶ä¸»é¢˜...")
                await asyncio.sleep(1)
                yield make_msg("log", level="success", message="ğŸ“‹ [Planner] ç”Ÿæˆäº† 3 ä¸ªæœç´¢å…³é”®è¯")
                
                yield make_msg("stage", stage="searching")
                yield make_msg("log", level="info", message="ğŸ” [Searcher] å¼€å§‹æœç´¢ç¬”è®°...")
                await asyncio.sleep(1)
                stats["notesFound"] = 15
                yield make_msg("stats", stats=stats)
                yield make_msg("log", level="success", message=f"ğŸ” [Searcher] æ”¶é›†äº† {stats['notesFound']} ç¯‡ç¬”è®°")
                
                yield make_msg("stage", stage="analyzing")
                yield make_msg("log", level="info", message="ğŸ§  [Analyzer] åˆ†ææ•°æ®ä¸­...")
                await asyncio.sleep(1)
                stats["contentsAnalyzed"] = 15
                stats["insightsExtracted"] = 8
                yield make_msg("stats", stats=stats)
                yield make_msg("log", level="success", message=f"ğŸ§  [Analyzer] æå–äº† {stats['insightsExtracted']} æ¡æ ¸å¿ƒå‘ç°")
                
                yield make_msg("stage", stage="generating")
                yield make_msg("log", level="info", message="ğŸ“ [Writer] ç”Ÿæˆç ”ç©¶æŠ¥å‘Š...")
                await asyncio.sleep(1)
                yield make_msg("log", level="success", message="âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
                yield make_msg("complete")
                return
            
            # è¿æ¥MCP
            yield make_msg("log", level="info", message="ğŸ“¡ è¿æ¥å°çº¢ä¹¦MCPæœåŠ¡...")
            await _mcp_client.connect()
            yield make_msg("log", level="success", message="âœ… MCPè¿æ¥æˆåŠŸ")
            
            # åˆ›å»ºç¼–æ’å™¨
            orchestrator = ResearchOrchestrator(_config, _mcp_client)
            
            # æ‰§è¡Œç ”ç©¶
            state = ResearchState(task=research_topic)
            
            # é˜¶æ®µ1: è§„åˆ’
            yield make_msg("log", level="info", message="ğŸ“‹ [Planner] åˆ†æç ”ç©¶ä¸»é¢˜...")
            state = await orchestrator.planner.run(state)
            if state.plan:
                yield make_msg("log", level="success", message=f"ğŸ“‹ [Planner] ç”Ÿæˆäº† {len(state.plan.keywords)} ä¸ªæœç´¢å…³é”®è¯")
                for kw in state.plan.keywords:
                    yield make_msg("log", level="info", message=f"  - {kw}")
            
            # é˜¶æ®µ2: æœç´¢
            yield make_msg("stage", stage="searching")
            yield make_msg("log", level="info", message="ğŸ” [Searcher] å¼€å§‹æœç´¢ç¬”è®°...")
            state = await orchestrator.searcher.run(state)
            stats["notesFound"] = len(state.documents)
            yield make_msg("stats", stats=stats)
            yield make_msg("log", level="success", message=f"ğŸ” [Searcher] æ”¶é›†äº† {stats['notesFound']} ç¯‡ç¬”è®°")
            
            # é˜¶æ®µ3: åˆ†æ
            yield make_msg("stage", stage="analyzing")
            yield make_msg("log", level="info", message="ğŸ§  [Analyzer] åˆ†ææ•°æ®ä¸­...")
            state = await orchestrator.analyzer.run(state)
            stats["contentsAnalyzed"] = len(state.documents)
            if state.insights:
                findings = state.insights.get("key_findings", [])
                stats["insightsExtracted"] = len(findings)
                yield make_msg("stats", stats=stats)
                yield make_msg("log", level="success", message=f"ğŸ§  [Analyzer] æå–äº† {stats['insightsExtracted']} æ¡æ ¸å¿ƒå‘ç°")
            
            # é˜¶æ®µ4: ç”Ÿæˆç»“æ„åŒ–å¤§çº²
            yield make_msg("stage", stage="generating")
            yield make_msg("log", level="info", message="ğŸ“‘ [OutlineGenerator] ç”Ÿæˆç»“æ„åŒ–å¤§çº²...")
            
            from ..output.outline_generator import OutlineGenerator
            outline_generator = OutlineGenerator(_config.get_llm_client(), model=_config.llm.model)
            
            try:
                structured_outline = await outline_generator.generate(state)
                yield make_msg("log", level="success", message=f"ğŸ“‘ [OutlineGenerator] ç”Ÿæˆäº† {len(structured_outline)} ä¸ªç« èŠ‚")
            except Exception as e:
                yield make_msg("log", level="warning", message=f"âš  å¤§çº²ç”Ÿæˆå¤±è´¥: {str(e)[:100]}, ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                structured_outline = outline_generator._generate_fallback_outline(state)
            
            yield make_msg("log", level="info", message="ğŸ“ [Writer] ç”Ÿæˆå›¾æ–‡äº¤é”™æŠ¥å‘Š...")
            html_generator = HTMLReportGenerator(_config.get_llm_client(), model=_config.llm.model)
            
            try:
                html_report = await html_generator.generate(state)
            except Exception as e:
                yield make_msg("log", level="warning", message=f"âš  LLMç”Ÿæˆå¤±è´¥: {str(e)[:100]}, ä½¿ç”¨å¤‡ç”¨æ¨¡æ¿")
                html_report = html_generator.generate_fallback_html(state)
            
            yield make_msg("log", level="success", message="âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
            
            # ä¼ é€’æŠ¥å‘Šæ•°æ®ç»™å‰ç«¯ï¼ˆåŒ…å«ç»“æ„åŒ–å¤§çº²ï¼‰
            report_data = {
                "topic": research_topic,
                "insights": state.insights,
                "outline": structured_outline,  # æ–°å¢ï¼šç»“æ„åŒ–å¤§çº²
                "notes": [
                    {
                        "id": note.preview.id,
                        "title": note.detail.title or note.preview.title,
                        "content": (note.detail.content or note.preview.content_preview)[:500],
                        "author": note.detail.author or note.preview.author,
                        "likes": note.detail.likes or note.preview.likes,
                        "images": note.detail.images[:3] if note.detail.images else [],
                        "url": note.detail.url or note.preview.url
                    }
                    for note in state.documents[:10]
                ]
            }
            yield make_msg("report", **report_data)
            
            final_status = "completed"
            history_service.update(record_id, {
                "status": "completed",
                "notes_count": stats["notesFound"],
                "sections_count": stats["insightsExtracted"]
            })
            yield make_msg("complete")
            
        except Exception as e:
            yield make_msg("log", level="error", message=f"âŒ ç ”ç©¶å¤±è´¥: {str(e)}")
            history_service.update(record_id, {"status": "failed"})
            yield make_msg("complete")
        
        finally:
            # æ–­å¼€MCPè¿æ¥
            if _mcp_client:
                try:
                    await _mcp_client.disconnect()
                except:
                    pass
    
    return EventSourceResponse(event_generator())


@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "ok",
        "mcp_configured": _mcp_client is not None,
        "timestamp": datetime.now().isoformat()
    }


# ========== è®¾ç½® API ==========

class LLMTestRequest(BaseModel):
    """LLM æµ‹è¯•è¯·æ±‚"""
    apiKey: str
    baseUrl: str
    model: str


@app.get("/api/settings")
async def get_settings():
    """è·å–è®¾ç½®ï¼ˆè„±æ•ï¼‰"""
    service = get_settings_service()
    return service.get_masked()


@app.post("/api/settings")
async def save_settings(settings: Settings):
    """ä¿å­˜è®¾ç½®"""
    service = get_settings_service()
    service.save(settings)
    return {"status": "ok", "message": "è®¾ç½®å·²ä¿å­˜"}


@app.post("/api/settings/test")
async def test_llm_connection(request: LLMTestRequest):
    """æµ‹è¯• LLM è¿æ¥"""
    from openai import AsyncOpenAI
    
    try:
        client = AsyncOpenAI(
            api_key=request.apiKey,
            base_url=request.baseUrl
        )
        
        # å‘é€ç®€å•æµ‹è¯•è¯·æ±‚
        response = await client.chat.completions.create(
            model=request.model,
            messages=[{"role": "user", "content": "Hi"}],
            max_tokens=5
        )
        
        return {"status": "ok", "message": "è¿æ¥æˆåŠŸ"}
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"è¿æ¥å¤±è´¥: {str(e)}")


# ========== å†å²è®°å½• API ==========

from ..services.history import get_history_service, ResearchRecord


@app.get("/api/history")
async def list_history(
    page: int = Query(1, ge=1),
    page_size: int = Query(10, ge=1, le=50),
    status: Optional[str] = None
):
    """è·å–å†å²è®°å½•åˆ—è¡¨"""
    service = get_history_service()
    return service.list(page=page, page_size=page_size, status=status)


@app.get("/api/history/{record_id}")
async def get_history_record(record_id: str):
    """è·å–å•ä¸ªå†å²è®°å½•"""
    service = get_history_service()
    record = service.get(record_id)
    if not record:
        raise HTTPException(status_code=404, detail="è®°å½•ä¸å­˜åœ¨")
    return record.model_dump()


@app.delete("/api/history/{record_id}")
async def delete_history_record(record_id: str):
    """åˆ é™¤å†å²è®°å½•"""
    service = get_history_service()
    if service.delete(record_id):
        return {"status": "ok", "message": "å·²åˆ é™¤"}
    raise HTTPException(status_code=404, detail="è®°å½•ä¸å­˜åœ¨")


@app.get("/api/history/search")
async def search_history(
    keyword: str = Query(..., min_length=1),
    limit: int = Query(10, ge=1, le=50)
):
    """æœç´¢å†å²è®°å½•"""
    service = get_history_service()
    results = service.search(keyword, limit)
    return [r.model_dump() for r in results]


# ========== å›¾ç‰‡éªŒè¯ API ==========

class ImageValidateRequest(BaseModel):
    """å›¾ç‰‡éªŒè¯è¯·æ±‚"""
    image_url: str
    context: str
    topic: str = ""


@app.post("/api/validate-image")
async def validate_image(request: ImageValidateRequest):
    """éªŒè¯å›¾ç‰‡ä¸å†…å®¹çš„ç›¸å…³æ€§"""
    from ..agents.image_validator import ImageValidator
    
    validator = ImageValidator()
    try:
        result = await validator.validate(
            image_source=request.image_url,
            context=request.context,
            topic=request.topic
        )
        return result.model_dump()
    finally:
        await validator.close()


class BatchImageValidateRequest(BaseModel):
    """æ‰¹é‡å›¾ç‰‡éªŒè¯è¯·æ±‚"""
    images: list[dict]  # [{url: str, caption?: str}, ...]
    context: str
    topic: str = ""


@app.post("/api/validate-images")
async def validate_images_batch(request: BatchImageValidateRequest):
    """æ‰¹é‡éªŒè¯å›¾ç‰‡"""
    from ..agents.image_validator import validate_images_batch
    
    results = await validate_images_batch(
        images=request.images,
        context=request.context,
        topic=request.topic
    )
    return results


# ============ è®¾ç½®ç›¸å…³ API ============

def get_effective_config() -> Config:
    """
    è·å–æœ‰æ•ˆé…ç½®ï¼ˆUI è®¾ç½®ä¼˜å…ˆï¼Œç„¶å ENVï¼‰
    
    ä¼˜å…ˆçº§ï¼š
    1. UI è®¾ç½®ï¼ˆå¦‚æœé…ç½®äº† API Keyï¼‰
    2. ç¯å¢ƒå˜é‡é…ç½®
    """
    config = Config.from_env()
    ui_settings = get_settings_service().load()
    
    # UI è®¾ç½®è¦†ç›– ENVï¼ˆå¦‚æœ UI è®¾ç½®äº† API Keyï¼‰
    if ui_settings.llm.api_key:
        config.llm.api_key = ui_settings.llm.api_key
        config.llm.base_url = ui_settings.llm.base_url
        config.llm.model = ui_settings.llm.model
    
    return config


@app.get("/api/settings")
async def get_settings():
    """è·å–è„±æ•åçš„è®¾ç½®"""
    return get_settings_service().get_masked()


class SettingsUpdateRequest(BaseModel):
    """è®¾ç½®æ›´æ–°è¯·æ±‚"""
    llm: dict = {}
    vlm: dict = {}
    imageGen: dict = {}


@app.post("/api/settings")
async def save_settings(data: SettingsUpdateRequest):
    """ä¿å­˜è®¾ç½®"""
    from ..services.settings import LLMSettings, VLMSettings, ImageGenSettings
    
    # è½¬æ¢å­—æ®µåï¼ˆå‰ç«¯ä½¿ç”¨ camelCaseï¼Œåç«¯ä½¿ç”¨ snake_caseï¼‰
    llm_data = {
        "api_key": data.llm.get("apiKey", ""),
        "base_url": data.llm.get("baseUrl", "https://api.openai.com/v1"),
        "model": data.llm.get("model", "gpt-4o")
    }
    vlm_data = {
        "enabled": data.vlm.get("enabled", False),
        "api_key": data.vlm.get("apiKey", ""),
        "base_url": data.vlm.get("baseUrl", ""),
        "model": data.vlm.get("model", "")
    }
    image_gen_data = {
        "enabled": data.imageGen.get("enabled", False),
        "api_key": data.imageGen.get("apiKey", ""),
        "base_url": data.imageGen.get("baseUrl", ""),
        "model": data.imageGen.get("model", "")
    }
    
    settings = Settings(
        llm=LLMSettings(**llm_data),
        vlm=VLMSettings(**vlm_data),
        imageGen=ImageGenSettings(**image_gen_data)
    )
    get_settings_service().save(settings)
    return {"success": True, "message": "è®¾ç½®å·²ä¿å­˜"}


class LLMTestRequest(BaseModel):
    """LLM è¿æ¥æµ‹è¯•è¯·æ±‚"""
    apiKey: str
    baseUrl: str
    model: str


@app.post("/api/settings/test")
async def test_llm_connection(data: LLMTestRequest):
    """æµ‹è¯• LLM è¿æ¥"""
    from openai import AsyncOpenAI
    
    try:
        client = AsyncOpenAI(
            api_key=data.apiKey,
            base_url=data.baseUrl
        )
        response = await client.chat.completions.create(
            model=data.model,
            messages=[{"role": "user", "content": "Hello"}],
            max_tokens=5
        )
        return {
            "success": True, 
            "message": f"è¿æ¥æˆåŠŸï¼æ¨¡å‹: {response.model}"
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


# ========== SPA è·¯ç”±ï¼ˆå¿…é¡»åœ¨æ‰€æœ‰ API è·¯ç”±ä¹‹åæ³¨å†Œï¼‰==========

_static_dir = Path(__file__).parent / "static"
if _static_dir.exists() and (_static_dir / "index.html").exists():
    @app.get("/{full_path:path}", include_in_schema=False)
    async def serve_spa(full_path: str):
        """SPA å…œåº•è·¯ç”±ï¼šè¿”å› index.html æˆ–é™æ€æ–‡ä»¶"""
        # API è·¯ç”±ä¸å¤„ç†ï¼ˆæ­£å¸¸æƒ…å†µä¸‹ä¸ä¼šåˆ°è¿™é‡Œï¼Œå› ä¸º API è·¯ç”±å·²å…ˆæ³¨å†Œï¼‰
        if full_path.startswith("api/"):
            raise HTTPException(status_code=404, detail="API not found")
        
        # æ£€æŸ¥æ˜¯å¦è¯·æ±‚é™æ€æ–‡ä»¶
        requested_file = _static_dir / full_path
        if requested_file.exists() and requested_file.is_file():
            return FileResponse(str(requested_file))
        
        # å…¶ä»–æ‰€æœ‰è¯·æ±‚è¿”å› index.htmlï¼ˆSPA è·¯ç”±ï¼‰
        return FileResponse(str(_static_dir / "index.html"))


# ç›´æ¥è¿è¡Œå…¥å£
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
