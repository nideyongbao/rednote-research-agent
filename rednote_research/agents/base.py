"""æ™ºèƒ½ä½“åŸºç±»"""

import asyncio
import logging
from abc import ABC, abstractmethod
from datetime import datetime
from typing import TYPE_CHECKING, Callable, Optional
from openai import AsyncOpenAI

if TYPE_CHECKING:
    from ..state import ResearchState

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class BaseAgent(ABC):
    """
    æ‰€æœ‰æ™ºèƒ½ä½“çš„åŸºç±»
    
    å®šä¹‰äº†æ™ºèƒ½ä½“çš„åŸºæœ¬ç»“æ„ï¼š
    - åç§°å’Œç³»ç»Ÿæç¤ºè¯
    - LLMè°ƒç”¨æ¥å£ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    - æ—¥å¿—å›è°ƒ
    """
    
    # é‡è¯•é…ç½®
    MAX_RETRIES = 3
    INITIAL_RETRY_DELAY = 1.0  # åˆå§‹å»¶è¿Ÿï¼ˆç§’ï¼‰
    MAX_RETRY_DELAY = 10.0  # æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰
    
    def __init__(
        self, 
        name: str, 
        llm_client: AsyncOpenAI,
        system_prompt: str,
        model: str
    ):
        """
        åˆå§‹åŒ–æ™ºèƒ½ä½“
        
        Args:
            name: æ™ºèƒ½ä½“åç§°
            llm_client: OpenAIå®¢æˆ·ç«¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            model: ä½¿ç”¨çš„æ¨¡å‹
        """
        self.name = name
        self.llm = llm_client
        self.system_prompt = system_prompt
        self.model = model
    
    @abstractmethod
    async def run(
        self, 
        state: "ResearchState",
        on_log: Optional[Callable[[str], None]] = None
    ) -> "ResearchState":
        """
        æ‰§è¡Œæ™ºèƒ½ä½“é€»è¾‘
        
        Args:
            state: å…±äº«çŠ¶æ€å¯¹è±¡
            on_log: æ—¥å¿—å›è°ƒå‡½æ•°
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€å¯¹è±¡
        """
        pass
    
    async def _invoke_llm(
        self, 
        messages: list[dict],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        top_p: Optional[float] = None,
        presence_penalty: Optional[float] = None,
        on_log: Optional[Callable[[str], None]] = None
    ) -> str:
        """
        è°ƒç”¨LLMå¹¶è¿”å›å“åº”ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°ï¼ˆNoneåˆ™ä½¿ç”¨settingsé»˜è®¤å€¼ï¼‰
            max_tokens: æœ€å¤§tokenæ•°ï¼ˆNoneåˆ™ä½¿ç”¨settingsé»˜è®¤å€¼ï¼‰
            top_p: top_pé‡‡æ ·å‚æ•°
            presence_penalty: å­˜åœ¨æƒ©ç½šå‚æ•°
            on_log: æ—¥å¿—å›è°ƒ
            
        Returns:
            LLMå“åº”æ–‡æœ¬
        """
        # ä»settingsè·å–é»˜è®¤å‚æ•°
        from ..services.settings import get_settings_service
        settings = get_settings_service().load()
        
        # ä½¿ç”¨settingsä¸­çš„é»˜è®¤å€¼
        temperature = temperature if temperature is not None else settings.llm.temperature
        max_tokens = max_tokens if max_tokens is not None else settings.llm.max_tokens
        top_p = top_p if top_p is not None else settings.llm.top_p
        presence_penalty = presence_penalty if presence_penalty is not None else settings.llm.presence_penalty
        
        last_exception = None
        
        for attempt in range(self.MAX_RETRIES):
            try:
                start_time = datetime.now()
                
                # æ„å»ºè¯·æ±‚å‚æ•°
                request_params = {
                    "model": self.model,
                    "messages": messages,
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                    "top_p": top_p,
                }
                
                # æ·»åŠ å¯é€‰å‚æ•°
                if presence_penalty is not None:
                    request_params["presence_penalty"] = presence_penalty
                
                # æ—¥å¿—ï¼šè¯·æ±‚å¼€å§‹
                logger.info(
                    f"[{self.name}] LLMè¯·æ±‚å¼€å§‹ | å°è¯• {attempt + 1}/{self.MAX_RETRIES} | "
                    f"æ¨¡å‹: {self.model} | temperature: {temperature} | max_tokens: {max_tokens}"
                )
                
                response = await self.llm.chat.completions.create(**request_params)
                
                # è®¡ç®—è€—æ—¶
                elapsed = (datetime.now() - start_time).total_seconds()
                
                # æ£€æŸ¥å“åº”æœ‰æ•ˆæ€§
                if response is None:
                    raise RuntimeError("LLM è¿”å›ç©ºå“åº”")
                if not response.choices:
                    raise RuntimeError(f"LLM æ²¡æœ‰è¿”å› choices: {response}")
                
                content = response.choices[0].message.content or ""
                
                # æ—¥å¿—ï¼šè¯·æ±‚æˆåŠŸ
                usage = response.usage
                log_msg = (
                    f"LLMè¯·æ±‚æˆåŠŸ | è€—æ—¶: {elapsed:.2f}s | "
                    f"è¾“å…¥: {usage.prompt_tokens if usage else 'N/A'} | "
                    f"è¾“å‡º: {usage.completion_tokens if usage else 'N/A'}"
                )
                logger.info(f"[{self.name}] {log_msg} | å“åº”é•¿åº¦: {len(content)}å­—ç¬¦")
                
                if on_log:
                    on_log(f"ğŸ¤– {log_msg}")
                
                return content
                
            except Exception as e:
                last_exception = e
                
                # è®¡ç®—é€€é¿å»¶è¿Ÿï¼ˆæŒ‡æ•°é€€é¿ï¼‰
                delay = min(
                    self.INITIAL_RETRY_DELAY * (2 ** attempt),
                    self.MAX_RETRY_DELAY
                )
                
                # æ—¥å¿—ï¼šè¯·æ±‚å¤±è´¥
                logger.warning(
                    f"[{self.name}] LLMè¯·æ±‚å¤±è´¥ | å°è¯• {attempt + 1}/{self.MAX_RETRIES} | "
                    f"é”™è¯¯: {type(e).__name__}: {str(e)[:100]} | "
                    f"{'å°†åœ¨ %.1fs åé‡è¯•' % delay if attempt < self.MAX_RETRIES - 1 else 'å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°'}"
                )
                
                if attempt < self.MAX_RETRIES - 1:
                    await asyncio.sleep(delay)
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        logger.error(
            f"[{self.name}] LLMè¯·æ±‚æœ€ç»ˆå¤±è´¥ | å·²é‡è¯• {self.MAX_RETRIES} æ¬¡ | "
            f"æœ€åé”™è¯¯: {type(last_exception).__name__}: {last_exception}"
        )
        raise last_exception
    
    def _log(
        self, 
        state: "ResearchState", 
        message: str,
        on_log: Optional[Callable[[str], None]] = None,
        level: str = "info"
    ):
        """
        è®°å½•æ—¥å¿—
        
        Args:
            state: ç ”ç©¶çŠ¶æ€
            message: æ—¥å¿—æ¶ˆæ¯
            on_log: æ—¥å¿—å›è°ƒ
            level: æ—¥å¿—çº§åˆ« (info/warning/error)
        """
        full_message = f"[{self.name}] {message}"
        state.add_log(full_message)
        
        # è¾“å‡ºåˆ°æ ‡å‡†æ—¥å¿—
        log_func = getattr(logger, level, logger.info)
        log_func(full_message)
        
        if on_log:
            on_log(full_message)
